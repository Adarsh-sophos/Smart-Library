# Smart Library
> Library Inventory Building
> Retrieval System

- Managing books in a large bookshelf and ﬁnding books on it often leads to tedious manual work, especially for large book collections where books might be missing or misplaced.
- Manually searching bookshelves is time-consuming and often not fruitful depending on how vague the search is.
-  The intent of this system is to make what was previously a tedious experience (i.e., searching books in bookshelves) much more user-friendly.
- Recently, deep neural models, such as Convolutional Neural Networks (CNN) and Recurrent Neural Networks (RNN) have achieved great success for scene text detection and recognition.
- Our system has two goals:
  1. Build a book inventory from only the images of bookshelves.
  2. Help users quickly locate the book they are looking for.


# Ideas
- Use online amazon database to store recognized data and fetch data.
- Can use string pattern matching for correcting book names. For it, we have to make a book database containing all books name OR use spelling correction or nearest Levenshtein distance matching against a dictionary to correct spellings.
- After spine recognition, the user can select individual spines in the phone’s viewﬁnder to obtain more information about a particular book, without ever having to take that book off the bookshelf.
- We can also have a database of images of book spine for every book and then use some image similarity measures between images, already stored in database and taken from a camera.
- Use the digital compass and accelerometer on the smartphone to estimate location of the identified books. The digital compass tells us which direction the phone is facing when a book is photographed, while a temporal trace of the accelerometer informs us how far vertically and horizontally the phone has moved from the last anchor point.
- Once the book is identiﬁed, additional stored information could also be retrieved, e.g. a short description of the book.
- Indexing and searching from a book database.
- Find the location of books on the bookshelves.


# Book Spine Segmentation
Book spine segmentation is a critical component of our system since each book is expected to be recognized, stored, and queried independently. 

## First Approach (Low-level Feature Based)

## Second Approach (CNN Based)
Performance of most existing approaches is limited by book spine segmentation. Hand-craft features based book spine segmentation suffers from common image distortion and low contrast between books.
- Use Hough Transform as a pre-processing step to extract the dominant direction of books in the image.
- Dominant direction is then used to rotate the entire image.
- Then apply a text/non-text CNN model trained on 32×64 color image patches to generate saliency maps of the rotated image.
- The saliency images could be further used in the following ways:
  1. Extract the book title location
  2. Segment each book.
- Use non-max suppression to ﬁnd the segmenting point of each book along the vertical axis.
- Here, we circumvent the need for book spine segmentation methods based on the Hough Transform or other low-level routines, which can be easily influenced by lighting conditions or low contrast between books.

# Book Retrieval
Two approaches -
1. Image feature-based recognition pipeline - The image features of the book spine image are searched through a book spine image database.
2. Text-based recognition pipeline - The text within the book spine image is recognized and used as keywords to search a book spine text database.

# 1. Image Feature-based Retrieval (Book Spine Recognition)
- Feature-based book spine recognition
- Match the segmented spine against an online database of book spines.

# 2. Text-based Retrieval (Text Recognition)
## 2.1 Text Localization
A scene text detection algorithm is applied to each book spine our system obtains. This step detects words on the book spines, which provide detailed and useful information about the book.
For text localization in a library environment, we use a book spine segmentation method based on Hough transform and scene text saliency. A state-of-the-art text localization method is subsequently applied to extract multi-oriented text information on a segmented book spine.
- Use region proposal based method for scene text detection.
- First generate extreme region because of fast computation and high recall.
- Saliency maps generated by the CNN are then used to ﬁlter out non-text regions.
- A multi-orientation text line grouping algorithm is applied to ﬁnd different lines of text - by ﬁrst constructing a character graph and then aligning character components into text lines.
- Low level features, such as perceptual divergence and aspect ratio, are used to ﬁnd text components that belong to the same line.
- We need to further decide whether a text line is upside down or not. To address this issue, train a CNN classiﬁer on 32 × 96 image patches.
-  The binary classiﬁer tells us whether we need to flip the text lines in order to provide the text recognition module with a correct sequence.

## 2.2 Text Recognition
In this system, book spine images are identiﬁed based on the recognized text, which are then used as keywords for indexing or searching a book database. During the querying process, system only relies on text information without requiring the original spine images. 

### 2.2.1 OCR Based Approach
- OCR system, such as Tesseract (Smith, 2007), perform poorly on image taken of natural scenes

### 2.2.2 Deep Learning Based Approach
Build a deep neural network-based scene text reading system. For text recognition, adopt a deep sequential labeling model based on convolutional and recurrent neural architectures.
Use a per-timestep classiﬁcation loss in tandem with a revised version of the **Connectionist Temporal Classiﬁcation (CTC)** (Graves et al., 2006) loss, which accelerates the training process and improves the performance.

#### 2.2.2.1 Text Recognition via Sequence Labeling
- Segmenting and recognizing each character is highly sensitive to various distortions in images, making character level segmentation imperfect and sometimes even impossible.
- So we train a model to recognize a sequence of characters simultaneously.
- Adopt a hybrid approach that combines a CNN with an RNN, casting scene text recognition problem as a sequential labeling task.
-  A CNN at the bottom learns features from the images which are then composed as feature sequences that are subsequently fed into an RNN for sequential labeling.
- A bidirectional Long Short-Term Memory (B-LSTM) model is applied on top of the learned sequential CNN features to exploit the interdependence among features.

#### 2.2.2.2 CTC Loss
- Sequences X (prediction) and Y (target) have diﬀerent lengths, so we adopt CTC loss to allow an RNN to be trained  for sequence labeling task without exact alignment.
- CTC loss is the negative log likelihood of outputting a target word Y given an input sequence X.
- Stochastic gradient descent (SGD) method is used for optimization.
- Use forward backward dynamic programming method for computation of loss.
- Decoding (ﬁnding the most likely Y from the output sequence X) can be done by performing a beam search  or simply by selecting the single most likely prediction at each timestep and then applying the mapping function B.

#### 2.2.2.3 Text Correction
- A post-processing step is necessary to correct the outputs of the recognition model.
- Experiments shows that that a standard spellchecker is not powerful enough for automatic correction, since we need to not only correct individual words, but also break larger strings into words.
- So train another RNN, employing a character-level sequence-to-sequence model.
- The RNN is designed with 2 layers of LSTM, each with a hidden size of 512.
- Both recognition model and correction model is trained using mini-batch stochastic gradient descent (SGD) together with Adadelta (Zeiler, 2012).


## 2.3 Text Search
- Make a book spine text database using inverted ﬁles, commonly used in text retrieval systems.
- Two approaches to ﬁnd matching words
  1. Exact word matching
  2. Nearest neighbor word matching
- Use tf-idf (term frequency-inverse document frequency). tf weights the word according to the number of occurrences within the spine text, and idf weights the score based on the how many different titles the word has occurred in.


# Combining both the Results


# Location Tracker
- Determine the locations of individual books, specifying which room, which bookshelf, and where in the bookshelf each recognized book can be found.

# Experiments
-  During search, use tf-idf (term frequency-inverse document frequency) weights to rank returned results.
-  Build search engine based on Apache Solr so that system scales well to large collection of books.
-  Evaluate results using Reciprocal Rank (RR) which is deﬁned as: RR = 1/K, where K is the rank position of the ﬁrst relevant document (the target book spine in our case).
-  Average Reciprocal Rank (MRR) across multiple queries should also be reported. All these measures are widely used by the information retrieval community.
-  Since the book collection contains books with very similar titles, even using groundtruth titles as keywords in search cannot guarantee 100% recall at top-1 rank position.

#### Probelms
-  Wrong predictions due to less discriminative keywords used during search. Multiple books may have very similar or even identical titles.
-  Some meta information on book spine images tends to be blurry and small in size, making detection and recognition more difﬁcult.
  -  Although image-based search might address this issue, it comes with the steep cost of storing and transmitting images.
-  Other failure cases are majorly due to imperfect or even wrong text localization bounding boxes.
